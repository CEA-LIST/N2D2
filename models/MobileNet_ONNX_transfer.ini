;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;    (C) Copyright 2017 CEA LIST. All Rights Reserved.
;    Contributor(s): Olivier BICHLER (olivier.bichler@cea.fr)
;
;    This software is governed by the CeCILL-C license under French law and
;    abiding by the rules of distribution of free software.  You can  use,
;    modify and/ or redistribute the software under the terms of the CeCILL-C
;    license as circulated by CEA, CNRS and INRIA at the following URL
;    "http://www.cecill.info".
;
;    As a counterpart to the access to the source code and  rights to copy,
;    modify and redistribute granted by the license, users are provided only
;    with a limited warranty  and the software's author,  the holder of the
;    economic rights,  and the successive licensors  have only  limited
;    liability.
;
;    The fact that you are presently reading this means that you have had
;    knowledge of the CeCILL-C license and that you accept its terms.
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

; Run an ONNX model inside N2D2

; ./n2d2.sh "$N2D2_MODELS/MobileNet_ONNX.ini" -seed 1 -test -w /dev/null

$BATCH_SIZE=256

DefaultModel=Frame_CUDA

; Database
[database]
;Type=ILSVRC2012_Database
;RandomPartitioning=0
;Learn=1.0
;BackgroundClass=1  ; Necessary for Google MobileNet pre-trained models
Type=CIFAR100_Database
Validation=0.1

; Environment (data pre-processing)
; Pre-processing is usually not included in the ONNX models available online.
; In the case of MobileNet, the standard ImageNet pre-processing must be added
; in order to reproduce the model reported accuracy.
; This can be easily done by adding N2D2 pre-processing blocks before importing
; the ONNX model.
; It is necessary to define the correct pre-processing and reproduce the 
; intended accuracy before running an export, in order for the calibration
; process to work on the right data distribution and therefore achieve the best
; accuracy after quantization.
[sp]
SizeX=224
SizeY=224
NbChannels=3
BatchSize=${BATCH_SIZE}

[sp.Transformation-1]
Type=RescaleTransformation
Width=256
Height=256

[sp.Transformation-2]
Type=PadCropTransformation
Width=224
Height=224

[sp.Transformation-3]
Type=ColorSpaceTransformation
ColorSpace=RGB

[sp.Transformation-4]
Type=RangeAffineTransformation
FirstOperator=Minus
FirstValue=127.5
SecondOperator=Divides
SecondValue=127.5

; Here, we insert an ONNX graph in the N2D2 flow the same way as a regular Cell
[onnx]
Input=sp
Type=ONNX
File=mobilenet_v1_1.0_224.onnx
; Remove the last layer and the softmax for transfer learning
Ignore=Conv__252 MobilenetV1/Predictions/Softmax

; The next two sections below are only necessary for fine-tuning the ONNX
; layers in N2D2 (see the detailed explanation at the end of this file)

; Default section for ONNX Conv from section "onnx"
; "ConfigSection", solvers and fillers can be specified here...
[onnx:Conv_def]
ConfigSection=common.config

; Default section for ONNX Fc from section "onnx"
[onnx:Fc_def]
ConfigSection=common.config

; For BatchNorm, make sure the stats won't change if there is no fine-tuning
; on these layers
[onnx:BatchNorm_def]
ConfigSection=bn_notrain.config
[bn_notrain.config]
MovingAverageMomentum=0.0

;[onnx:Conv__250]
;ConfigSection=common.config,notrain.config
;[notrain.config]
;BackPropagate=0

; Here, we add our new layers for transfer learning
[fc]
; first input MUST BE "onnx" for proper dependency handling
Input=onnx,MobilenetV1/Logits/AvgPool_1a/AvgPool:0
Type=Fc
NbOutputs=100
ActivationFunction=Linear
WeightsFiller=XavierFiller
ConfigSection=common.config

[softmax]
Input=fc
Type=Softmax
NbOutputs=[fc]NbOutputs
WithLoss=1

[softmax.Target]

; Common config for static model
[common.config]
WeightsSolver.LearningRate=0.01
WeightsSolver.Momentum=0.9
WeightsSolver.Decay=0.0005
Solvers.LearningRatePolicy=StepDecay
Solvers.LearningRateStepSize=[sp]_EpochSize
Solvers.LearningRateDecay=0.993

; For transfer learning, we can either:
; * fine tune the full network along with the new fully connected layer.
;   In this case, one must make sure that the solver parameters are correctly
;   specified also for the ONNX layers. This is done above with the 
;   [onnx:Conv_def] and [onnx:Fc_def] sections, which ensure that the parameters
;   in the config section [common.config] are used for all the layers.
;   Please note that if these are missing, the default N2D2 solver parameters
;   will be used, as the ONNX model never contain the training parameters.
;   Score: ~72.5%
;
; * fine tune only the new fully connected layer and the preceding Conv__250
;   layer
;   Score: ~69.5%
;
; * or only train the new fully connected layer.
;   In this case, one must ensures that the preceding layers (in the ONNX model)
;   are not updated, by uncommenting the following line:
;   Score: ~58%
BackPropagate=0
