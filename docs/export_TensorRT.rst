Export: TensorRT
================

Export type: ``CPP_TensorRT``

C++ export using TensorRT.

.. code-block::

    n2d2 MobileNet_ONNX.ini -seed 1 -w /dev/null -export CPP_TensorRT -nbbits 32
    cd export_CPP_TensorRT_float32
    make

.. Warning::

    The calibration for this export is done using the tools provided by NVIDIA. For this reason, you cannot calibrate when exporting your network.
    You need to use the export to calibrate your network.

The TensorRT API export can run the generated program in NVIDIA GPU
architecture. It use CUDA, cuDNN and TensorRT API library. All the
native TensorRT layers are supported. The export support from TensorRT
2.1 to TensorRT 5.0 versions.

Program options related to the TensorRT API export:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

+--------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Option [default value]   | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
+==========================+===============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================+
| ``-batch`` [1]           | Size of the batch to use                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
+--------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| ``-dev`` [0]             | CUDA Device ID selection                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
+--------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| ``-stimulus`` [NULL]     | Path to a specific input stimulus to test. For example: -stimulus :math:`{/stimulus/env0000.pgm}` command will test the file env0000.pgm of the stimulus folder.                                                                                                                                                                                                                                                                                                                                              |
+--------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| ``-prof``                | Activates the layer wise profiling mechanism. This option can decrease execution time performance.                                                                                                                                                                                                                                                                                                                                                                                                            |
+--------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| ``-iter-build`` [1]      | Sets the number of minimization build iterations done by the tensorRT builder to find the best layer tactics.                                                                                                                                                                                                                                                                                                                                                                                                 |
+--------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| ``-nbbits`` [-32]        | Number of bits used for computation. Value -32 for Full FP32 bits configuration, -16 for Half FP16 bits configuration and 8 for INT8 bits configuration. When running INT8 mode for the first time, the TensorRT calibration process can be very long. Once generated the generated calibration table will be automatically reused. Supported compute mode in function of the compute capability are provided here: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities .   |
+--------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

Example
~~~~~~~

Test the exported network with layer wise profiling:
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

::

    ./bin/n2d2_tensorRT_test -prof

The results of the layer wise profiling should look like:

.. code-block:: console

    (19%)  **************************************** CONV1 + CONV1_ACTIVATION: 0.0219467 ms
    (05%)  ************ POOL1: 0.00675573 ms
    (13%)  **************************** CONV2 + CONV2_ACTIVATION: 0.0159089 ms
    (05%)  ************ POOL2: 0.00616047 ms
    (14%)  ****************************** CONV3 + CONV3_ACTIVATION: 0.0159713 ms
    (19%)  **************************************** FC1 + FC1_ACTIVATION: 0.0222242 ms
    (13%)  **************************** FC2: 0.0149013 ms
    (08%)  ****************** SOFTMAX: 0.0100633 ms
    Average profiled tensorRT process time per stimulus = 0.113932 ms


Calibrate the exported network :
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

::

    ./bin/n2d2_tensorRT_test -nbbits 8
