

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Post-training quantization &mdash; N2D2  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Quantization aware training" href="quant_qat.html" />
    <link rel="prev" title="Train from ONNX models" href="onnx_transfer.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> N2D2
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Introduction:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="intro.html">Presentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="about.html">About N2D2-IP</a></li>
<li class="toctree-l1"><a class="reference internal" href="simus.html">Performing simulations</a></li>
<li class="toctree-l1"><a class="reference internal" href="perfs_tools.html">Performance evaluation tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="tuto.html">Tutorials</a></li>
</ul>
<p class="caption"><span class="caption-text">ONNX Import:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="onnx_convert.html">Obtain ONNX models</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx_import.html">Import ONNX models</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx_transfer.html">Train from ONNX models</a></li>
</ul>
<p class="caption"><span class="caption-text">Quantization and Export:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Post-training quantization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#principle">Principle</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#weights-normalization">1) Weights normalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#activations-normalization">2) Activations normalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quantization">3) Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#additional-optimization-strategies">Additional optimization strategies</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#weights-clipping-optional">Weights clipping (optional)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#activation-scaling-factor-approximation">Activation scaling factor approximation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#usage-in-n2d2">Usage in N2D2</a></li>
<li class="toctree-l2"><a class="reference internal" href="#examples-and-results">Examples and results</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="quant_qat.html">Quantization aware training</a></li>
<li class="toctree-l1"><a class="reference internal" href="export_CPP.html">Export: C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="export_TensorRT.html">Export: TensorRT</a></li>
<li class="toctree-l1"><a class="reference internal" href="export_DNeuro.html">Export: DNeuro</a></li>
<li class="toctree-l1"><a class="reference internal" href="export_legacy.html">Export: other / legacy</a></li>
</ul>
<p class="caption"><span class="caption-text">INI File Interface:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="ini_intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="ini_databases.html">Databases</a></li>
<li class="toctree-l1"><a class="reference internal" href="ini_data_analysis.html">Stimuli data analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="ini_environment.html">Stimuli provider (Environment)</a></li>
<li class="toctree-l1"><a class="reference internal" href="ini_layers.html">Network Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="ini_target.html">Targets (outputs &amp; losses)</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="containers.html">Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="cells.html">Cells</a></li>
<li class="toctree-l1"><a class="reference internal" href="databases.html">Databases</a></li>
<li class="toctree-l1"><a class="reference internal" href="stimuliprovider.html">StimuliProvider</a></li>
<li class="toctree-l1"><a class="reference internal" href="deepnet.html">DeepNet</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">N2D2</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Post-training quantization</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/quant_post.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="post-training-quantization">
<h1>Post-training quantization<a class="headerlink" href="#post-training-quantization" title="Permalink to this headline">¶</a></h1>
<div class="section" id="principle">
<h2>Principle<a class="headerlink" href="#principle" title="Permalink to this headline">¶</a></h2>
<p>The post-training quantization algorithm is done in 3 steps:</p>
<div class="section" id="weights-normalization">
<h3>1) Weights normalization<a class="headerlink" href="#weights-normalization" title="Permalink to this headline">¶</a></h3>
<p>All weights are rescaled in the range <span class="math notranslate nohighlight">\([-1.0, 1.0]\)</span>.</p>
<dl class="simple">
<dt>Per layer normalization</dt><dd><p>There is a single weights scaling factor, global to the layer.</p>
</dd>
<dt>Per layer and per output channel normalization</dt><dd><p>There is a different weights scaling factor for each output channel. This allows
a finer grain quantization, with a better usage of the quantized range for some
output channels, at the expense of more factors to be saved in memory.</p>
</dd>
</dl>
</div>
<div class="section" id="activations-normalization">
<h3>2) Activations normalization<a class="headerlink" href="#activations-normalization" title="Permalink to this headline">¶</a></h3>
<p>Activations at each layer are rescaled in the range <span class="math notranslate nohighlight">\([-1.0, 1.0]\)</span> for signed
outputs and <span class="math notranslate nohighlight">\([0.0, 1.0]\)</span> for unsigned outputs.</p>
<p>The <strong>optimal quantization threshold value</strong> of the activation output of each
layer is determined using the validation dataset (or test dataset if no
validation dataset is available).</p>
<p>This is an iterative process: need to take into account previous layers
normalizing factors.</p>
<p>Finding the optimal quantization threshold value of the activation output of
each layer is done the following:</p>
<ol class="arabic simple">
<li><p>Compute histogram of activation values;</p></li>
<li><p>Find threshold that minimizes distance between original distribution and
clipped quantized distribution. Two distance algorithms can be used:</p>
<ul class="simple">
<li><p>Mean Squared Error (MSE);</p></li>
<li><p>Kullback–Leibler divergence metric (KL-divergence).</p></li>
</ul>
</li>
</ol>
<div class="figure align-default">
<img alt="Activation values histogram and corresponding thresholds." src="_images/activations_histogram.png" />
</div>
<p>The obtained threshold value is therefore the activation scaling factor to be
taken into account during quantization.</p>
</div>
<div class="section" id="quantization">
<h3>3) Quantization<a class="headerlink" href="#quantization" title="Permalink to this headline">¶</a></h3>
<p>Inputs, weights, biases and activations are quantized to the desired
<span class="math notranslate nohighlight">\(nbbits\)</span> precision.</p>
<p>Convert ranges from <span class="math notranslate nohighlight">\([-1.0, 1.0]\)</span> and <span class="math notranslate nohighlight">\([0.0, 1.0]\)</span> to
<span class="math notranslate nohighlight">\([-2^{nbbits-1}-1, 2^{nbbits-1}-1]\)</span> and <span class="math notranslate nohighlight">\([0, 2^{nbbits}-1]\)</span> taking
into account all dependencies.</p>
</div>
<div class="section" id="additional-optimization-strategies">
<h3>Additional optimization strategies<a class="headerlink" href="#additional-optimization-strategies" title="Permalink to this headline">¶</a></h3>
<div class="section" id="weights-clipping-optional">
<h4>Weights clipping (optional)<a class="headerlink" href="#weights-clipping-optional" title="Permalink to this headline">¶</a></h4>
<p>Weights can be clipped using the same strategy than for the activations (
finding the optimal quantization threshold using the weights histogram).
However, this usually leads to worse results than no clipping.</p>
</div>
<div class="section" id="activation-scaling-factor-approximation">
<h4>Activation scaling factor approximation<a class="headerlink" href="#activation-scaling-factor-approximation" title="Permalink to this headline">¶</a></h4>
<p>The activation scaling factor <span class="math notranslate nohighlight">\(\alpha\)</span> can be approximated the following
ways:</p>
<ul class="simple">
<li><p>Fixed-point: <span class="math notranslate nohighlight">\(\alpha\)</span> is approximated by <span class="math notranslate nohighlight">\(x 2^{-p}\)</span>;</p></li>
<li><p>Single-shift: <span class="math notranslate nohighlight">\(\alpha\)</span> is approximated by <span class="math notranslate nohighlight">\(2^{x}\)</span>;</p></li>
<li><p>Double-shift: <span class="math notranslate nohighlight">\(\alpha\)</span> is approximated by <span class="math notranslate nohighlight">\(2^{n} + 2^{m}\)</span>.</p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="usage-in-n2d2">
<h2>Usage in N2D2<a class="headerlink" href="#usage-in-n2d2" title="Permalink to this headline">¶</a></h2>
<p>All the post-training strategies described above are available in N2D2 for any
export type. To apply post-training quantization during export, simply use the
<code class="docutils literal notranslate"><span class="pre">-calib</span></code> command line argument.</p>
<p>The following parameters are available in command line:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 27%" />
<col style="width: 73%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Argument [default value]</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-calib</span></code></p></td>
<td><p>Number of stimuli used for the calibration (<code class="docutils literal notranslate"><span class="pre">-1</span></code> = use the full validation dataset)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-calib-reload</span></code></p></td>
<td><p>Reload and reuse the data of a previous calibration</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-wt-clipping-mode</span></code> [<code class="docutils literal notranslate"><span class="pre">None</span></code>]</p></td>
<td><p>Weights clipping mode on export, can be <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">MSE</span></code> or <code class="docutils literal notranslate"><span class="pre">KL-Divergence</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-act-clipping-mode</span></code> [<code class="docutils literal notranslate"><span class="pre">MSE</span></code>]</p></td>
<td><p>Activations clipping mode on export, can be <code class="docutils literal notranslate"><span class="pre">None</span></code>, <code class="docutils literal notranslate"><span class="pre">MSE</span></code> or <code class="docutils literal notranslate"><span class="pre">KL-Divergence</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">-act-rescaling-mode</span></code> [<code class="docutils literal notranslate"><span class="pre">Single-shift</span></code>]</p></td>
<td><p>Activations scaling mode on export, can be <code class="docutils literal notranslate"><span class="pre">Floating-point</span></code>, <code class="docutils literal notranslate"><span class="pre">Fixed-point</span></code>, <code class="docutils literal notranslate"><span class="pre">Single-shift</span></code> or <code class="docutils literal notranslate"><span class="pre">Double-shift</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">-act-rescale-per-output</span></code> [0]</p></td>
<td><p>If true (1), rescale activation per output instead of per layer</p></td>
</tr>
</tbody>
</table>
<p>Command line example to run the C++ Export on a INI file containing an ONNX
model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n2d2</span> <span class="n">MobileNet_ONNX</span><span class="o">.</span><span class="n">ini</span> <span class="o">-</span><span class="n">seed</span> <span class="mi">1</span> <span class="o">-</span><span class="n">w</span> <span class="o">/</span><span class="n">dev</span><span class="o">/</span><span class="n">null</span> <span class="o">-</span><span class="n">export</span> <span class="n">CPP</span> <span class="o">-</span><span class="n">fuse</span> <span class="o">-</span><span class="n">calib</span> <span class="o">-</span><span class="mi">1</span> <span class="o">-</span><span class="n">act</span><span class="o">-</span><span class="n">clipping</span><span class="o">-</span><span class="n">mode</span> <span class="n">KL</span><span class="o">-</span><span class="n">Divergence</span>
</pre></div>
</div>
</div>
<div class="section" id="examples-and-results">
<h2>Examples and results<a class="headerlink" href="#examples-and-results" title="Permalink to this headline">¶</a></h2>
<p>Post-training quantization accuracy obtained with some models from the ONNX
Model Zoo are reported in the table below, using <code class="docutils literal notranslate"><span class="pre">-calib</span> <span class="pre">1000</span></code>:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 56%" />
<col style="width: 11%" />
<col style="width: 20%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><em>ONNX Model Zoo</em> model (specificities)</p></th>
<th class="head"><p>FP acc.</p></th>
<th class="head"><p>Fake 8 bits acc.</p></th>
<th class="head"><p>8 bits acc.</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>resnet18v1.onnx
(<code class="docutils literal notranslate"><span class="pre">-no-unsigned</span> <span class="pre">-act-rescaling-mode</span> <span class="pre">Fixed-point</span></code>)</p></td>
<td><p>69.83%</p></td>
<td><p>68.82%</p></td>
<td><p>68.78%</p></td>
</tr>
<tr class="row-odd"><td><p>mobilenetv2-1.0.onnx
(<code class="docutils literal notranslate"><span class="pre">mobilenetv20_output_flatten0_reshape0</span></code> ignored)</p></td>
<td><p>70.95%</p></td>
<td><p>65.40%</p></td>
<td><p>65.40%</p></td>
</tr>
<tr class="row-even"><td><p>mobilenetv2-1.0.onnx
(<code class="docutils literal notranslate"><span class="pre">mobilenetv20_output_flatten0_reshape0</span></code> ignored
<code class="docutils literal notranslate"><span class="pre">-act-rescaling-mode</span> <span class="pre">Fixed-point</span></code>)</p></td>
<td></td>
<td><p>66.67%</p></td>
<td><p>66.70%</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p><em>FP acc.</em> is the floating point accuracy obtained before post-training
quantization on the model imported in ONNX;</p></li>
<li><p><em>Fake 8 bits acc.</em> is the accuracy obtained after post-training quantization
in N2D2, in fake-quantized mode (the numbers are quantized but the
representation is still floating point);</p></li>
<li><p><em>8 bits acc.</em> is the accuracy obtained after post-training quantization in the
N2D2 reference C++ export, in actual 8 bits representation.</p></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="quant_qat.html" class="btn btn-neutral float-right" title="Quantization aware training" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="onnx_transfer.html" class="btn btn-neutral float-left" title="Train from ONNX models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, CEA LIST

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>